---
title: "New_Andrew_PA"
format: html
editor: visual
---

# Code for generating principal components, based on food data.

> Loading in John's data:


```{r, message=FALSE}
library(tidyverse)
library(stringr)
library(nnet)
library(dplyr)
library(caret)
load("../data/tech_data.Rdata")
```


## Creating dataset

Currently:
- Population is all adults over 45 who responded to the survey on BOTH days.
    - Motivation for this is that metabolism generally decreases with age... probably good to consult the nutrition students, though.
- Food codes have been replaced with their actual labels in the data dictionary.

```{r}

fooditems_dict_major = read.csv("../data/nutmstatDataItems2019_CSV.csv") |>
  filter(str_length(Category.Code) == 3) |>
  dplyr::rename("THRDIG" = "Category.Code") |>
  dplyr::rename("Category.Label.Major" = "Category.Label")

fooditems_dict_minor = read.csv("../data/nutmstatDataItems2019_CSV.csv") |>
  filter(str_length(Category.Code) == 5) |>
  dplyr::rename("FIVDIG" = "Category.Code") |>
  dplyr::rename("Category.Label.Minor" = "Category.Label")

both_days_adults_food <- tech_food |>
  group_by(ABSPID) |>
  left_join(y = tech_biom |> select(ABSPID, AGEC, HYPBC, BDYMSQ04), by = "ABSPID") |>
  filter(n_distinct(DAYNUM) == 2, AGEC >= 45, HYPBC == 5, BDYMSQ04 == 5) |>
  ungroup() |>
  left_join(fooditems_dict_major, by = "THRDIG") |>
  left_join(fooditems_dict_minor, by = "FIVDIG") |>
  select(ABSPID, Category.Label.Major, Category.Label.Minor, GRAMWGT)

head(both_days_adults_food, 10)
```


### (NEW) Splitting necessary major groups by minor groups
```{r}
complex_group_splits <- both_days_adults_food |>
  mutate(Category.Label.Major = 
           ifelse(Category.Label.Major == "Potatoes",
             ifelse(Category.Label.Minor == "Potato products", "Fried Potatoes and chips", "Potatoes and potato dishes"), 
           Category.Label.Major)) |>
  mutate(Category.Label.Major = 
           ifelse(Category.Label.Major == "Mixed dishes where cereal is the major ingredient",
                  ## then
                  case_when(
                    str_detect(Category.Label.Minor, regex("pizza", ignore_case = TRUE)) ~ "Pizza",
                    str_detect(Category.Label.Minor, regex("burgers", ignore_case = TRUE)) ~ "Burgers",
                    str_detect(Category.Label.Minor, regex("sushi", ignore_case = TRUE)) ~ "Sushi",
                    str_detect(Category.Label.Minor, regex("saturated fat >5 g/100 g", ignore_case = TRUE)) ~ "High-fat cereal-based takeaway-style food",
                    # LOW FAT TAKEAWAY: Note: R does not recognise ≤ sign. All higher fat cases captured above.
                    str_detect(Category.Label.Minor, regex("5 g/100 g", ignore_case = TRUE)) | Category.Label.Minor == "Other savoury grain dishes" ~ "Low-fat cereal-based takeaway-style food",
                    .default = "Gnocchi and steamed buns"
                    ),
                  Category.Label.Major
          )
         ) |>
  select(-Category.Label.Minor) |>
  dplyr::rename("Category.Label" = "Category.Label.Major")
```


### Creating a "servings" measure based on grams
Looking at the food data, we can create define "1 serving" as the median grams consumed of a particular item.

From here, we just compute servings as (using tea as an example):

$$\text{Servings(Tea)} = \frac{\text{Recorded tea weight (g)}}{\text{Median tea weight (g)}}$$

By comparing grams to "servings", we can directly compare different types of food consumed, without having to worry about how different foods are usually consumed in different weights.

```{r}
both_days_adults_food_servings <- complex_group_splits |>
  group_by(Category.Label) |> 
  mutate(median_grams = median(GRAMWGT)) |>
  mutate(Servings = ifelse(median_grams != 0, GRAMWGT / median_grams, 0)) # sets to 0 if median grams is 0... avoids any divide by 0 errors.

both_days_adults_food_servings$Servings <- format(both_days_adults_food_servings$Servings, scientific = FALSE)
head(both_days_adults_food_servings, 10)
```

#### Summarising data such that we have one row per individual

Each row is summed servings of food that person consumed over two days.

```{r}
wide_adults_food_servings = both_days_adults_food_servings |>
  select(-GRAMWGT, -median_grams) |>
  mutate(Servings = as.numeric(Servings)) |>
  group_by(ABSPID, Category.Label) |>
  summarise(Total_Servings = sum(Servings, na.rm = TRUE), .groups = 'drop') |>
  pivot_wider(names_from = Category.Label, values_from = Total_Servings, values_fill = 0)

head(wide_adults_food_servings, 10)
```

#### Grouping food items together with substantial overlap

Merging together overlapping food items (columns) aids w factor analysis.

**Note:** Ingredients with minimal nutritional content (e.g. baking soda, herbs) and/or low count will be removed.

```{r}
wide_adults_food_servings_foods_grouped <- wide_adults_food_servings |>
  # Removing variables with minimal nutritional content and/or rare groups, which don't fit in with anything else.
  select(-c(
    `Herbs, spices, seasonings and stock cubes`, # this also includes added salt..? Should be careful.
    `Essences`,
    `Stuffings`,
    `Chemical raising agents and cooking ingredients`,
    `Infant foods`,
    `Infant cereal products`,
    `Cheese substitute`,
    `Soy-based ice confection`,
    `Dry soup mix`,
    `Soy-based yoghurts`,
    `Formula dietary foods`
    )) |>
  # Soups
  mutate(Soups = rowSums(across(matches("Soup", ignore.case = TRUE)), na.rm = TRUE)) |>
  select(-matches("Soup"), Soups) |>
  # Fish
  mutate(`Fish and Fish Products` = rowSums(across(c(
    `Other sea and freshwater foods`, 
    `Crustacea and molluscs (excluding commercially sterile)`, 
    `Fish and seafood products (homemade and takeaway)`,
    `Mixed dishes with fish or seafood as the major component`,
    `Packed (commercially sterile) fish and seafood`,
    `Fin fish (excluding commercially sterile)`
    )), na.rm = TRUE)) |>
  select(-c(
    `Other sea and freshwater foods`, 
    `Crustacea and molluscs (excluding commercially sterile)`, 
    `Fish and seafood products (homemade and takeaway)`,
    `Mixed dishes with fish or seafood as the major component`,
    `Packed (commercially sterile) fish and seafood`,
    `Fin fish (excluding commercially sterile)`)) |>
  # Fruit
  mutate(`Fresh Fruit and Fruit-based Dishes` = rowSums(across(c(
    `Berry fruit`, 
    `Stone fruit`,
    `Citrus fruit`,
    `Other fruit`,
    `Pome fruit`,
    `Tropical and subtropical fruit`,
    `Mixtures of two or more groups of fruit`
    )))) |>
  select(-`Berry fruit`, -`Stone fruit`, -`Citrus fruit`, -`Other fruit`, -`Pome fruit`, -`Tropical and subtropical fruit`, -`Mixtures of two or more groups of fruit`) |>
  # Fruit and muesli bars
  mutate(`Fruit and Muesli-style bars` = rowSums(across(c(
    `Fruit, nut and seed-bars`,
    `Muesli or cereal style bars`)))) |>
  select(-c(`Fruit, nut and seed-bars`, 
            `Muesli or cereal style bars`)) |>
  # Cordials and soft drink
  mutate(`Soft drinks, Cordials, Sports drinks, and Milkshakes` = rowSums(across(c(
    `Soft drinks, and flavoured mineral waters`, 
    `Cordials`, 
    `Flavoured milks and milkshakes`, 
    `Electrolyte, energy and fortified drinks`)))) |>
  select(-c(`Soft drinks, and flavoured mineral waters`, 
            `Cordials`, 
            `Flavoured milks and milkshakes`, 
            `Electrolyte, energy and fortified drinks`)) |>
  # Desserts
  mutate(`Milk-based and other dessert items` = rowSums(across(c(
    `Other dishes where milk or a milk product is the major component`, 
    `Sweet biscuits`, 
    `Frozen milk products`, 
    `Dishes and products other than confectionery where sugar is the major component`, 
    `Custards`,
    `Batter-based products`,
    `Mixed dishes where fruit is the major component`,
    `Cakes, muffins, scones, cake-type desserts`)))) |>
  select(-c(`Other dishes where milk or a milk product is the major component`, 
            `Sweet biscuits`, 
            `Frozen milk products`, 
            `Dishes and products other than confectionery where sugar is the major component`,
            `Mixed dishes where fruit is the major component`,
            `Custards`, `Batter-based products`, `Cakes, muffins, scones, cake-type desserts`)) |>
  # Chocolate and lollies
  mutate(`Chocolate and lollies` = rowSums(across(c(
    `Chocolate and chocolate-based confectionery`, 
    `Other confectionery`)))) |>
  select(-c(`Chocolate and chocolate-based confectionery`, 
            `Other confectionery`)) |>
  # Butters, margarines, and oils
  mutate(`Butters, margarines, and oils` = rowSums(across(c(
    `Butters`,
    `Margarine and table spreads`, 
    `Plant oils`,
    `Unspecified fats`,
    `Dairy blends`,
    `Other fats`)))) |>
  select(-c(`Butters`, `Margarine and table spreads`, `Plant oils`, `Unspecified fats`, `Dairy blends`, `Other fats`)) |>
  # Sauces
  mutate(`Sauces` = rowSums(across(c(`Gravies and savoury sauces`, `Salad dressings`)))) |>
  select(-c(`Gravies and savoury sauces`, `Salad dressings`)) |>
  # Coffee and Tea
  mutate(`Coffee and Tea` = rowSums(across(c(`Coffee and coffee substitutes`, `Tea`)))) |>
  select(-c(`Coffee and coffee substitutes`, `Tea`)) |>
  # Starchy snacks
  mutate(`Starchy snacks` = rowSums(across(c(
    `Extruded or reformed snacks`,
    `Other snacks`,
    `Corn snacks`,
    `Potato snacks`
    )))) |>
  select(-c(
    `Extruded or reformed snacks`,
    `Other snacks`,
    `Corn snacks`,
    `Potato snacks`)) |>
  # Alcohol
  mutate(`Alcohol` = rowSums(across(c(
    `Wines`,
    `Beers`,
    `Spirits`,
    `Cider and perry`,
    `Other alcoholic beverages`
  )))) |>
  select(-c(
    `Wines`,
    `Beers`,
    `Spirits`,
    `Cider and perry`,
    `Other alcoholic beverages`
  )) |>
  # Legumes
  mutate(`Legumes and pulses` = rowSums(across(c(
    `Peas and beans`,
    `Mature legumes and pulses`,
    `Mature legume and pulse products and dishes`
  )))) |>
  select(-c(
    `Peas and beans`,
    `Mature legumes and pulses`,
    `Mature legume and pulse products and dishes`
  )) |>
  # Breakfast cereal and porridge
  mutate(`Breakfast cereal and porridge` = rowSums(across(c(
    `Breakfast cereals, ready to eat`,
    `Breakfast cereals, hot porridge style`
  )))) |>
  select(-c(
    `Breakfast cereals, ready to eat`,
    `Breakfast cereals, hot porridge style`)) |>
  # Cheese, yogurts
  mutate(`Cheese, yogurt, cream` = rowSums(across(c(
    `Cheese`,
    `Yoghurt`,
    `Cream`
  )))) |>
  select(-c(
    `Cheese`,
    `Yoghurt`,
    `Cream`
  )) |>
  # Processed meat
  mutate(`Processed meats` = rowSums(across(c(
    `Processed meat`,
    `Sausages, frankfurts and saveloys`
  )))) |>
  select(-c(
    `Processed meat`,
    `Sausages, frankfurts and saveloys`
  )) |>
  # Red meat, unprocessed
  mutate(`Red meat, unprocessed` = rowSums(across(c(
    `Beef, sheep and pork, unprocessed`,
    `Mammalian game meats`,
  )))) |>
  select(-c(
    `Beef, sheep and pork, unprocessed`,
    `Mammalian game meats`,
  )) |>
  # Eggs and egg dishes
  mutate(`Eggs and egg dishes` = rowSums(across(c(
    `Eggs`,
    `Dishes where egg is the major ingredient`,
  )))) |>
  select(-c(
    `Eggs`,
    `Dishes where egg is the major ingredient`,
  ))

# This is actually quite annoying... `mixed dishes where cereal is main component` includes sandwiches, sushi, pizza, etc... 

## Testing
colnames(wide_adults_food_servings_foods_grouped)
```

> Results of variable merging (so far).

```{r}
colnames(wide_adults_food_servings_foods_grouped)
```


## PCA

```{r}
pca_result <- prcomp(wide_adults_food_servings_foods_grouped |> select(-ABSPID), scale = TRUE)

summary(pca_result)

# This is what is used in regression.
pca_scores <- pca_result$x

# loadings (contributions of each variable to the components)
pca_loadings <- pca_result$rotation

# Plot the variance explained by each principal component
plot(pca_result, type = "l")
```

There's approx. 4 PC's with eigenvalue > 1.5.


## Trying to regression on the PCA data

```{r}
# Merging data frames so that we can do a regression on a variable that we want
pca_scores <- pca_result$x
pca_scores_w_id = data.frame(ABSPID = wide_adults_food_servings_foods_grouped$ABSPID, pca_scores)
pca_merged_with_biom_dat = merge(tech_biom, pca_scores_w_id, by = "ABSPID")
pca_merged_with_dat = merge(tech_nutr, pca_merged_with_biom_dat, by="ABSPID")
pca_merged_with_dat$ENERGTOT <- pca_merged_with_dat$ENERGYT1 + pca_merged_with_dat$ENERGYT2

pca_merged_with_biom_dat$ENERGTOT <- pca_merged_with_dat$ENERGTOT

# Cleaning the merged df
missing_percentage <- colMeans(is.na(pca_merged_with_biom_dat)) * 100
filtered_data <- pca_merged_with_biom_dat[, (missing_percentage < 44) | (names(pca_merged_with_biom_dat) == "DIAHBRSK") | (names(pca_merged_with_biom_dat) == "GLUCFPD") | (names(pca_merged_with_biom_dat) == "TRIGNTR") | (names(pca_merged_with_biom_dat) == "HCHOLBC ")]
cleaned_pca_merged <- filtered_data

# Fixing exercise variables to numeric
cleaned_pca_merged$EXLWTBC <- as.numeric(as.character(cleaned_pca_merged$EXLWTBC))
cleaned_pca_merged$EXLWMBC <- as.numeric(as.character(cleaned_pca_merged$EXLWMBC))
cleaned_pca_merged$EXLWVBC <- as.numeric(as.character(cleaned_pca_merged$EXLWVBC))

# Removing columns that end with "_MISS"
cols_to_remove <- grep("_MISS$", colnames(cleaned_pca_merged))
cleaned_pca_merged <- cleaned_pca_merged[, -cols_to_remove]

# Removing any zero variance variables
zero_var_columns <- nearZeroVar(cleaned_pca_merged, saveMetrics = TRUE)
cleaned_pca_merged <- cleaned_pca_merged[, !zero_var_columns$zeroVar]

cleaned_pca_merged <- cleaned_pca_merged %>% select(-ABSPID)

cleaned_pca_merged <- na.omit(cleaned_pca_merged)
cleaned_pca_merged <- droplevels(cleaned_pca_merged)

glimpse(cleaned_pca_merged)

```

```{r}
# Plot Functions
plot_lm_model <- function(formula, data, dependent_var_name) {
  # Fit the model
  model <- lm(formula, data = data)
  # Predicted vs Actual Plot
  predicted <- predict(model)
  actual <- data[[dependent_var_name]]  # Access the actual dependent variable
  
  # Plot Predicted vs Actual
  plot(actual, predicted, 
       main = paste("Predicted vs Actual for", dependent_var_name), 
       xlab = "Actual", 
       ylab = "Predicted")
  abline(0, 1, col = "red")
  
  # Residuals vs Fitted Plot
  plot(model, which = 1, main = paste("Residuals vs Fitted for", dependent_var_name))
  
  # Return the model summary for further inspection
  summary(model)$r.squared
  return(summary(model))
}

plot_quantile_regression_model <- function(formula, null_formula, data, dependent_var_name, tau = 0.5) {
  # Fit the quantile regression model
  model <- rq(formula, data = data, tau = tau)  # tau is the quantile level (default to median, i.e., 0.5)
  
  # Predicted vs Actual Plot
  predicted <- predict(model)
  actual <- data[[dependent_var_name]]  # Access the actual dependent variable
  
  # Plot Predicted vs Actual
  plot(actual, predicted, 
       main = paste("Predicted vs Actual for", dependent_var_name, "Quantile:", tau), 
       xlab = "Actual", 
       ylab = "Predicted")
  abline(0, 1, col = "red")
  
  # Residuals vs Fitted Plot
  residuals <- actual - predicted
  plot(predicted, residuals, 
       main = paste("Residuals vs Fitted for", dependent_var_name, "Quantile:", tau),
       xlab = "Fitted", 
       ylab = "Residuals")
  abline(h = 0, col = "red")
  
  # Making The Pseudo-R^2
  null_model <- rq(null_formula, data=data, tau=tau)
  resid_model <- sum(abs(residuals(model)))
  resid_null <- sum(abs(residuals(null_model)))
  
  pseudo_r2 <- 1 - (resid_model / resid_null)  # Pseudo-R² computation
  
  cat("Pseudo R-squared for quantile", tau, ":", pseudo_r2, "\n")
  # Return the model summary for further inspection
  return(summary(model))
}

multinom_logit_visualization <- function(formula, data, plot_type = c("coefficients", "probabilities", "marginal")) {
  
  # Fit the multinomial model
  model <- multinom(formula, data = data)
  
  # Switch based on plot type
  plot_type <- match.arg(plot_type)
  
  if (plot_type == "coefficients") {
    # Tidy the model for coefficient plot
    coef_df <- tidy(model)
    # Plot coefficients
    p <- dwplot(coef_df) + 
      theme_minimal() +
      labs(title = "Coefficient Plot for Multinomial Logistic Regression",
           x = "Estimate", y = "Variable")
    print(p)
  
  } else if (plot_type == "probabilities") {
    # Create new data for prediction
    predictors <- all.vars(formula)[2]  # Get the predictor variables from formula
    newdata <- expand.grid(lapply(data[predictors], function(x) seq(min(x), max(x), length.out = 100)))
    
    # Get predicted probabilities
    pred_probs <- predict(model, newdata = newdata, type = "probs")
    
    # Melt predicted probabilities for plotting
    pred_probs_melt <- melt(data.frame(newdata, pred_probs), id.vars = predictors)
    
    # Plot predicted probabilities
    p <- ggplot(pred_probs_melt, aes_string(x = names(newdata)[1], y = "value", color = "variable")) +
      geom_line() +
      theme_minimal() +
      labs(title = "Predicted Probabilities from Multinomial Logistic Regression",
           x = names(newdata)[1], y = "Probability", color = "Outcome")
    print(p)
    
  } else if (plot_type == "marginal") {
    # Plot marginal effects
    plot(allEffects(model))
  }
}

plot_multinom_model <- function(formula, data, outcome_var_name) {
  print(formula)
  print(colnames(data))  # Check if DIAHBRSK is really present
  
  # Fit the multinomial logistic regression model
  model <- multinom(formula, data = data)
  
  # Get the summary of the model
  model_summary <- summary(model)
  
  # Extract coefficients and standard errors
  coeff <- model_summary$coefficients
  std_err <- model_summary$standard.errors

  # Calculate z-values
  z_values <- coeff / std_err

  # Calculate p-values (two-tailed)
  p_values <- 2 * (1 - pnorm(abs(z_values)))

  # Return the summary with p-values
  return(list(summary = model_summary, z_values = z_values, p_values = p_values))
}

find_p_values <- function(model_summary){
  # Extract coefficients and standard errors
  coeff <- model_summary$coefficients
  std_err <- model_summary$standard.errors

  # Calculate z-values
  z_values <- coeff / std_err

  # Calculate p-values (two-tailed)
  p_values <- 2 * (1 - pnorm(abs(z_values)))
  return(p_values)
}
```


## Doing the Regression

```{r}
glimpse(cleaned_pca_merged)

# The Linear Regression
glimpse(cleaned_pca_merged)
cleaned_pca_merged$SM
plot_lm_model(PHDCMWBC ~ PC1 + PC2 + PC3 + ENERGTOT ,  cleaned_pca_merged, "PHDCMWBC")  # Waist Circumference
summary(lm(PHDCMWBC ~ . ,  cleaned_pca_merged))
plot_lm_model(PHDKGWBC ~ PC1 + PC2 + PC3, cleaned_pca_merged, "PHDKGWBC")  # Weight
plot_lm_model(SYSTOL ~ PC1 + PC2 + PC3, cleaned_pca_merged, "SYSTOL")      # Systolic Blood Pressure
plot_lm_model(DIASTOL ~ PC1 + PC2 + PC3, cleaned_pca_merged, "DIASTOL")    # Diastolic Blood Pressure
plot_lm_model(BMISC ~ PC1 + PC2 + PC3,  cleaned_pca_merged, "BMISC") # BMI
plot_lm_model(EXLWMBC  ~ PC1 + PC2 + PC3,  cleaned_pca_merged, "EXLWMBC") # Physical Activity
plot_lm_model(ADTOTSE ~ PC1 + PC2 + PC3,  cleaned_pca_merged, "ADTOTSE") # Physical Activity

cor(cleaned_pca_merged$DIASTOL, clenaed_pca_merged$PC1)

# Logistic Regressiosn
model1 <- multinom(DIAHBRSK ~ PC1 + PC2 + PC3 + PC4, data = cleaned_pca_merged)
model2 <- multinom(TRIGNTR ~ PC1 + PC2 + PC3 + PC4, data = cleaned_pca_merged)
model3 <- multinom(HCHOLBC ~ PC1 + PC2 + PC3 + PC4, data = cleaned_pca_merged)
model4 <- multinom(GLUCFPD ~ PC1 + PC2 + PC3 + PC4, data = cleaned_pca_merged)
summary(model1)
summary(model2)
summary(model3)
summary(model4)
find_p_values(summary(model1))
find_p_values(summary(model2))
find_p_values(summary(model3))
find_p_values(summary(model4))
table(cleaned_pca_merged$HCHOLBC)
```

## Trying Gradient Boosting Regression
```{r}
library(gbm)

gradient_boosting_regression_model <- gbm(
  formula = DIASTOL ~ PC1 + PC2 + PC3 + ENERGTOT + SMKSTAT + INCDEC + SEX + BMISC + AGEC + EXLWMBC,
  data = cleaned_pca_merged,
  distribution = "gaussian",
  n.trees=500,
  interaction.depth=3,
  shrinkage=0.01,
  cv.folds = 5,
  n.minobsinnode = 10,
  verbose=FALSE
)


best_iter <- gbm.perf(gradient_boosting_regression_model, method = "cv")

# Print the best number of trees
print(best_iter)


predictions <- predict(gradient_boosting_regression_model, newdata = cleaned_pca_merged, n.trees = best_iter)

# Calculate RMSE
rmse <- sqrt(mean((cleaned_pca_merged$DIASTOL - predictions)^2))
print(paste("RMSE:", rmse))

# Calculate R-squared
SST <- sum((cleaned_pca_merged$DIASTOL - mean(cleaned_pca_merged$DIASTOL))^2)
SSE <- sum((cleaned_pca_merged$DIASTOL - predictions)^2)
r_squared <- 1 - (SSE / SST)
print(paste("R-squared:", r_squared))
```

## SVM Stuff

```{r}
# SVM
library(e1071)
set.seed(1370)
model <- svm(DIASTOL ~ PC1 + PC2 + PC3 + ENERGTOT + SMKSTAT + INCDEC + SEX + AGEC + EXLWMBC, data = cleaned_pca_merged, type="eps-regression", kernel = "sigmoid") # Linear, Polynomial, radial, sigmoid


predictions <- predict(model, cleaned_pca_merged)

mse <- mean((cleaned_pca_merged$DIASTOL - predictions)^2)

# R-squared
rss <- sum((cleaned_pca_merged$DIASTOL - predictions)^2)
tss <- sum((cleaned_pca_merged$DIASTOL - mean(cleaned_pca_merged$DIASTOL))^2)
r_squared <- 1 - rss/tss

# Display the results
mse
r_squared

# Permutation Feature Importance
# Function to calculate MSE
mse_func <- function(true, predicted) {
  mean((true - predicted)^2)
}

# Get baseline MSE
original_predictions <- predict(model, cleaned_pca_merged)
original_mse <- mse_func(cleaned_pca_merged$DIASTOL, original_predictions)

# Initialize a vector to store feature importance scores
feature_importance <- numeric(ncol(cleaned_pca_merged) - 1)  # Subtract 1 to ignore the target variable

# Iterate through each feature (excluding the target variable)
for (i in 1:(ncol(cleaned_pca_merged) - 1)) {
  # Create a copy of the dataset
  permuted_data <- cleaned_pca_merged
  
  # Shuffle the i-th feature
  permuted_data[, i] <- sample(permuted_data[, i])
  
  # Make predictions on the permuted dataset
  permuted_predictions <- predict(model, permuted_data)
  
  # Calculate the new MSE
  permuted_mse <- mse_func(cleaned_pca_merged$DIASTOL, permuted_predictions)
  
  # Calculate importance as the change in MSE
  feature_importance[i] <- permuted_mse - original_mse
}

# Name the feature importance values based on the predictors
names(feature_importance) <- colnames(cleaned_pca_merged)[1:(ncol(cleaned_pca_merged) - 1)]

# Print the feature importance scores
print(feature_importance)

```

## Random Foresting

```{r}

library(randomForest)

# Train a random forest model
rf_model <- randomForest(DIASTOL ~ ., data = cleaned_pca_merged)

# Get feature importance
importance(rf_model)

# Plot feature importance
varImpPlot(rf_model)
```

